/* Dataset from https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008 

-Background-
Thirty Days-Hospital Readmission is an indicator of the quality of care in a hospital measured within 30 days after the patient is discharged from the hospital. 
Chronic diseases are associated with being one of the factors a patient can return to the hospital, one of which is diabetes. Therefore, an accurate prediction 
is needed to determine whether a patient needs to be readmission based on the medical recap at his first visit.

-Notes-
I used jupyter notebook and in this project I used 9 model machine learning type supervised learning, namely k-Nearest Neighbor (KNN), Decision Tree 
Classification, Random Forest Classification, Gradient Boosting Classification, Na√Øve Bayes Classification, Logistic Regression Classification, Support Vector 
Machine (SVM) Classification, Artificial Neural Network (ANN) and XGBoost

-Result-
The best prediction results obtained in Artificial Neural Network (ANN) sequentional API with performance reaching 25.040% in predicting the accepted readmission 
class and rejected readmission classes.*/

import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import warnings 
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import average_precision_score,confusion_matrix, accuracy_score, recall_score, precision_score,f1_score
from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest, mutual_info_classif
warnings.filterwarnings('ignore')

diabetic_data ='seaborn_data/diabetic_data.csv'
diabetic = pd.read_csv(diabetic_data)

diabetic=diabetic.replace("?",np.nan)

missing_values_count = diabetic.isnull().sum()
missing_values_count[:]

diabetic.info()

/*----------------------------------------------------------------------------------------------------------------------------------------------------*/

# <span style='color:blue'> EXPLORATORY DATA ANALYSIS </span>

### Target = Readmisi

plt.figure(figsize=(10,5))
sns.countplot(x='readmitted',data=diabetic);
print(diabetic.readmitted.value_counts())

diabetic['readmitted'] = diabetic['readmitted'].replace({"<30":"YES",
                                                         ">30":"YES"})
plt.figure(figsize=(10,5))
sns.countplot(x='readmitted',data=diabetic);
plt.title("Proportion of Target Value")
print(diabetic.readmitted.value_counts())

diabetic.readmitted.value_counts().plot.pie(autopct = "%.1f%%")
plt.title("Proportion of Target Value")
plt.show()

## Input Variable
### Gender

plt.figure(figsize=(10,5))
sns.countplot(x='gender',data=diabetic);
plt.title("Distribution of Gender")
print(diabetic.gender.value_counts())

/* unknown is not a gender, so I dropped it */
diabetic = diabetic.drop(diabetic.loc[diabetic["gender"]=="Unknown/Invalid"].index, axis=0)
plt.figure(figsize=(10,5))
sns.countplot(x='gender',hue='readmitted',data=diabetic);
plt.title("Distribution of Gender after drop unknown")
print(diabetic.gender.value_counts())

### Age

plt.figure(figsize=(10,5))
sns.countplot(x='age',data=diabetic);
plt.title("Age Distribution")
print(diabetic.age.value_counts())

diabetic['age']= diabetic['age'].replace({'[0-10)':5,
                                          '[10-20)':15,
                                          '[20-30)':25,
                                          '[30-40)':35,
                                          '[40-50)':45,
                                          '[50-60)':55,
                                          '[60-70)':65,
                                          '[70-80)':75,
                                          '[80-90)':85,
                                          '[90-100)':95})
plt.figure(figsize=(10,5))
sns.countplot(x='age',hue='readmitted',data=diabetic);
plt.title("Age Distribution")

### Admission Type ID

plt.figure(figsize=(10,5))
sns.countplot(x='admission_type_id',data=diabetic);
plt.title("Distribution of Admission IDs")
print(diabetic.admission_type_id.value_counts())

./*Notes for each number <br>
1: Emergency <br>
2: Urgent <br>
3: Elective <br>
4: Newborn <br>
5: Not Available <br>
6: NULL <br>
7: Trauma Center <br>
8: Not Mapped <br>
Data Not Available, NULL and Not better to be dropped <br>
urgent = emergency <br>
need mapping*/

mapping_admission_type_id = {1:"Emergency",
          2:"Emergency",
          3:"Elective",
          4:"New Born",
          5:np.nan,
          6:np.nan,
          7:"Trauma Center",
          8:np.nan}
diabetic.admission_type_id = diabetic.admission_type_id.replace(mapping_admission_type_id)
plt.figure(figsize=(10,5))
sns.countplot(x='admission_type_id',hue='readmitted',data=diabetic);
plt.title("Distribution of Admission IDs")
print(diabetic.admission_type_id.value_counts())

## Discharge Disposition ID

plt.figure(figsize=(10,5))
sns.countplot(x='discharge_disposition_id',data=diabetic);
plt.title("Distribution of Disposition IDs")

/*We classified patient discharge disposition as 4 part : discharge, expired, other and NaN <br>
No 1,2,3,4,5,6,8,15,16,17,22,23,24,27,28,29,30 = discharge <br>
No 7,9,10,12,13,14 = other <br>
No 11,19,20,21 = expired <br>
No 18,25,26 = NaN <br>
need maapping */

mapping_discharge_disposition_id = {1:"Discharged",2:"Discharged",3:"Discharged",4:"Discharged",5:"Discharged",6:"Discharged",8:"Discharged",
                                    15:"Discharged",16:"Discharged",17:"Discharged",
                                    22:"Discharged",23:"Discharged",24:"Discharged",27:"Discharged",28:"Discharged",29:"Discharged",30:"Discharged",
                                    7:"Other",9:"Other",10:"Other",12:"Other",13:"Other",14:"Other",
                                    11:"Expired",19:"Expired",20:"Expired",21:"Expired",
                                    18:np.nan,25:np.nan,26:np.nan}
diabetic.discharge_disposition_id = diabetic.discharge_disposition_id.replace(mapping_discharge_disposition_id)
plt.figure(figsize=(10,5))
sns.countplot(x='discharge_disposition_id',hue='readmitted',data=diabetic);
plt.title("Distribution of Disposition IDs")
print(diabetic.discharge_disposition_id.value_counts())

### Admission Source ID

plt.figure(figsize=(10,5))
sns.countplot(x='admission_source_id',data=diabetic);
plt.title("Distribution of Admission Source IDs")

/*For patient admission source divided into 4 :
No 1,2,3,4,5,6,10,18,22,25,26 = Referral & Transfer <br>
No 8,19 = Other <br>
No 9,15,17,20,21 = NaN <br>
No 11,12,13,14,23,24 = Birth <br>
most data value is emergency, so we prioritize emergency and not combine it with other data value <br>
No 7 = emergency <br>
need mapping*/

mapping_admission_source_id = {1:"Referral & Transfer",2:"Referral & Transfer",3:"Referral & Transfer",4:"Referral & Transfer",5:"Referral & Transfer",6:"Referral & Transfer",
                               10:"Referral & Transfer",18:"Referral & Transfer",22:"Referral & Transfer",25:"Referral & Transfer",26:"Referral & Transfer",
                               8:"Other",19:"Other",
                               9:np.nan,15:np.nan,17:np.nan,20:np.nan,21:np.nan,
                               11:"Birth",12:"Birth",13:"Birth",14:"Birth",23:"Birth",24:"Birth",
                               7:"Emergency" 
                              }
diabetic.admission_source_id = diabetic.admission_source_id.replace(mapping_admission_source_id)
plt.figure(figsize=(10,5))
sns.countplot(x='admission_source_id',hue='readmitted',data=diabetic);
plt.title("Distribution of Admission Source IDs")
print(diabetic.admission_source_id.value_counts())

### Diagnosis

/*each code show each type of data group (see data source journal for details) <br>
390-459,785 = Circulatory <br>
460-519,786 = Respiratory <br>
520-579,787 = Digestive <br>
250.xx = Diabetes <br>
800-999 = Injury <br>
710-739 = Musculoskeletal <br>
580-629,788 = Genitourinary <br>
140-239 = Neoplasms <br>
780,781,784,790-799, 240-278 tanpa 250, 680-709,782, 001-139,290-319,E-V,280-289,320-359,630-679,360-389,740-759 = Other*/

diabetic = kp.diabetic_diagnosis(diabetic,["diag_1","diag_2","diag_3"])

def plot_diabetic(col,data):
    sns.countplot(x=col,data=data,order=data[f"{col}"].value_counts().index)
    plt.xticks(rotation=90)
    plt.title(col)
    plt.show()
diag_cols=['diag_1','diag_2','diag_3']
for diag in diag_cols:
    plot_diabetic(diag,diabetic)

### Max Glu Serum

plt.figure(figsize=(10,5))
sns.countplot(x='max_glu_serum',data=diabetic);
print(diabetic.max_glu_serum.value_counts())

/*Have 4 values : ">200",">300","normal",and "none" <br>
combine ">200" and ">300" <br> 
none mean the patient has not yet to do the test*/

diabetic['max_glu_serum'] = diabetic['max_glu_serum'].replace({">300":">200"})
plt.figure(figsize=(10,5))
sns.countplot(x = "max_glu_serum",hue = "readmitted", data = diabetic)
plt.title("Glucose Serum Test Result")

### A1Cresult

plt.figure(figsize=(10,5))
sns.countplot(x='A1Cresult',data=diabetic);
print(diabetic.A1Cresult.value_counts())

diabetic = diabetic.replace({">8":">7"})
plt.figure(figsize=(10,5))
sns.countplot(x = "A1Cresult",hue="readmitted", data = diabetic)
plt.title("A1Cresult")
print(diabetic.A1Cresult.value_counts())

### Diabetes Medications

kolom_medicine=['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',
                'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',
                'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',
                'tolazamide', 'insulin', 'glyburide-metformin', 'glipizide-metformin',
                'glimepiride-pioglitazone','metformin-rosiglitazone', 'metformin-pioglitazone','examide','citoglipton']
def obat_obatan(medicines):
    for medicine in medicines:
        sns.countplot(x=medicine,hue='readmitted',data=diabetic)
        plt.show()
        print(medicine.upper())
        print(diabetic[f"{medicine}"].value_counts())
obat_obatan(kolom_medicine)

### Change

diabetic.change=diabetic.change.replace("Ch","Yes")
plt.figure(figsize=(10,5))
sns.countplot(x='change',hue='readmitted',data=diabetic);
plt.title("Change in Diabetic Medications")
print(diabetic.change.value_counts())

/*----------------------------------------------------------------------------------------------------------------------------------------------------*/

# <span style='color:blue'> Data Preprocessing </span>

diabetic.sort_values('patient_nbr',inplace=True)

diabetic.drop_duplicates(subset=['patient_nbr','readmitted'],keep=False,inplace=True)

### Drop Missing Value and unimportant data

feature_drop =['encounter_id','patient_nbr','weight','payer_code','medical_specialty','citoglipton']
diabetic=diabetic.drop(feature_drop,axis=1)
diabetic.head()

diabetic=diabetic.fillna(diabetic.mode().iloc[0])

diabetic.info()

plt.figure(figsize=(10,5))
sns.countplot(x='readmitted',data=diabetic);
print(diabetic.readmitted.value_counts())

### Normalisasi

num_features = ['age','time_in_hospital','num_lab_procedures','num_procedures','num_medications',
               'number_outpatient','number_emergency','number_inpatient','number_diagnoses']
diabetic_num = diabetic[num_features]

fig = plt.figure(figsize=(10,8))
ax = fig.gca()
diabetic_num.hist(ax=ax)
plt.show()

plt.figure(figsize=(20,5))
sns.boxplot(data=diabetic_num)

# perform a robust scaler transform of the dataset
trans = RobustScaler()
diabetic_num = trans.fit_transform(diabetic_num.values)

cat_features = ['race','gender','admission_type_id','discharge_disposition_id','admission_source_id',
               'diag_1','diag_2','diag_3','metformin','repaglinide','nateglinide','chlorpropamide',
               'glimepiride','acetohexamide','glipizide','glyburide','tolbutamide','pioglitazone',
               'rosiglitazone','acarbose','miglitol','troglitazone','tolazamide','examide','insulin',
               'glyburide-metformin','glipizide-metformin','glimepiride-pioglitazone','A1Cresult',
               'metformin-rosiglitazone','metformin-pioglitazone','change','diabetesMed','max_glu_serum']
diabetic_cat = diabetic[cat_features].values
diabetic_cat = diabetic_cat.astype(str)
print(diabetic_cat.shape)
type(diabetic_cat)

#Encode the categorical input using One Hot Encoding
onehot_encoder = OneHotEncoder(drop='first',sparse=False)
X_onehot_cat = onehot_encoder.fit_transform(diabetic_cat)
print('Input',X_onehot_cat.shape)
type(X_onehot_cat)

#### Merger Categorical & Numerical

all_data = np.concatenate((X_onehot_cat,diabetic_num),axis=1)
print(all_data)
print(all_data.shape)
type(all_data)

# save numpy array as csv file
from numpy import asarray
from numpy import savetxt
# define data
data = asarray(all_data)
# save to csv file
savetxt('seaborn_data/data.csv', data, delimiter=',')

# <span style='color:blue'> Train-Test Split </span>

target=diabetic.readmitted
target_map = target.map({"NO":0,"YES":1})
target_map = np.array(target_map)

X = all_data
y = target_map
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

#define number of features to evaluate
num_features= [i+1 for i in range(X.shape[1])]
print(num_features)

/* check test logistic regression accuracy using 1-98 parameters*/

X_train_mi,X_test_mi,fs_mi=kp.select_features_mi(X_train, y_train, X_test,'all')

#fit the model selected-features Mutual Infromation
model_mi =LogisticRegression(solver='saga')
model_mi.fit(X_train_mi, y_train)
#evaluate the model
yhat_mi=model_mi.predict(X_test_mi)
#evaluate predictions
accuracy = accuracy_score(y_test,yhat_mi)
fscore = f1_score(y_test,yhat_mi)
print('Accuracy: %.2f' % (accuracy*100))

for i in range(len(fs_mi.scores_)):
    print('Feature %d:%f'%(i,fs_mi.scores_[i]))

#plot the scores
plt.bar([i for i in range(len(fs_mi.scores_))],fs_mi.scores_)
plt.show()

# evaluate a given model using cross-validation
def evaluate_model(model):
    cv= RepeatedStratifiedKFold(n_splits=4, n_repeats=3, random_state=1)
    scores = cross_val_score (model, X_train,y_train, scoring='accuracy',cv=cv, n_jobs=-1)
    return scores
    
#find best k value
#enumerate each number of features
k_results=list()
for num_feature in num_features:
    # create pipeline
    model_mi = LogisticRegression(solver='saga')
    fs_mi = SelectKBest(score_func=mutual_info_classif,k=num_feature)
    pipeline_mi=Pipeline(steps=[('mi',fs_mi),('lr',model_mi)])
    # evaluate the model
    scores_mi=evaluate_model(pipeline_mi)
    k_results.append(scores_mi)
    # summarize the results
    print('>%d %.5f (%.5f)' % (num_feature, np.mean(scores_mi),np.std(scores_mi)))

/*Finally, I decided to used all the features because the result show that*/

/*----------------------------------------------------------------------------------------------------------------------------------------------------*/

# <span style='color:blue'> Machine Learning Model </span>

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import SGDClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.utils import plot_model
from tensorflow import keras

## <span style='color:red'>KNN</span>

acc_knn = list()
range_i = range(1,51)
for i in range_i :
    knn=KNeighborsClassifier(n_neighbors=i)
    #evaluate model
    scores_knn = evaluate_model(knn)
    acc_knn.append(np.mean(scores_knn)*100)
    print('>%d %.3f(%.3f)' % (i,np.mean(scores_knn)*100,np.std(scores_knn)))
    
plt.figure(figsize=(20,5))
knn_all_features = plt.plot(range(1,51),acc_knn,color='red',marker='o',markerfacecolor='blue',markersize=10)
plt.title("Accuracy Score KNN")
plt.xlabel("n_neighbors")
plt.ylabel("Score")
print("Max KNN :",max(acc_knn),"pada index ke-",acc_knn.index(max(acc_knn))+1)  

#test accuracy k=33
knn=KNeighborsClassifier(n_neighbors=33)
model_knn=knn.fit(X_train, y_train)
#evaluate the model
yhat_model_knn=model_knn.predict(X_test)
#evaluate predictions
accuracy_model_knn = accuracy_score(y_test,yhat_model_knn)
recall_model_knn = recall_score(y_test, yhat_model_knn)
precision_model_knn = precision_score(y_test, yhat_model_knn)
f1score_model_knn = f1_score(y_test, yhat_model_knn)
print('Accuracy  : %.3f' % (accuracy_model_knn*100))   
print('Recall    : %.3f' % (recall_model_knn*100))   
print('Precision : %.3f' % (precision_model_knn*100))   
print('F1Score   : %.3f' % (f1score_model_knn*100))

knn_cm = confusion_matrix(y_test,yhat_model_knn)
sns.heatmap(knn_cm,annot=True,fmt="d");

print(classification_report(y_test,yhat_model_knn))

## <span style='color:orange'>Decision Tree</span>

acc_dt = list()
range_i = range(1,51)
for i in range_i :
    dt=DecisionTreeClassifier(max_depth=i,random_state=1)
    #evaluate model
    scores_dt = evaluate_model(dt)
    acc_dt.append(np.mean(scores_dt)*100)
    print('>%d %.3f(%.3f)' % (i,np.mean(scores_dt)*100,np.std(scores_dt)))
    
plt.figure(figsize=(20,5))
dt_all_features = plt.plot(range(1,51),acc_dt,color='red',marker='o',markerfacecolor='blue',markersize=10)
plt.title("Accuracy Score Decision Tree")
plt.xlabel("max_depth")
plt.ylabel("Score")
print("Max Decission Tree :",max(acc_dt),"pada max_depth =",acc_dt.index(max(acc_dt))+1)
print("Max Decission Tree :",min(acc_dt),"pada max_depth =",acc_dt.index(min(acc_dt))+1)

#test accuracy k=3
dt=DecisionTreeClassifier(max_depth=3)
model_dt=dt.fit(X_train, y_train)
#evaluate the model
yhat_model_dt=model_dt.predict(X_test)
#evaluate predictions
accuracy_model_dt = accuracy_score(y_test,yhat_model_dt)
recall_model_dt = recall_score(y_test, yhat_model_dt)
precision_model_dt = precision_score(y_test, yhat_model_dt)
f1score_model_dt = f1_score(y_test, yhat_model_dt) 
print('Accuracy  : %.3f' % (accuracy_model_dt*100))   
print('Recall    : %.3f' % (recall_model_dt*100))   
print('Precision : %.3f' % (precision_model_dt*100))  
print('F1Score   : %.3f' % (f1score_model_dt*100))

decision_tree_cm = confusion_matrix(y_test,yhat_model_dt)
sns.heatmap(decision_tree_cm,annot=True,fmt="d");

print(classification_report(y_test,yhat_model_dt))

## <span style='color:yellow'>Random Forest</span>

acc_rf = list()
range_i = range(1,51)
for i in range_i :
    rf=RandomForestClassifier(max_depth=i,random_state=1)
    #evaluate model
    scores_rf = evaluate_model(rf)
    acc_rf.append(np.mean(scores_rf)*100)
    print('>%d %.3f(%.3f)' % (i,np.mean(scores_rf)*100,np.std(scores_rf)))
    
plt.figure(figsize=(20,5))
rf_all_features = plt.plot(range(1,51),acc_rf,color='red',marker='o',markerfacecolor='blue',markersize=10)
plt.title("Accuracy Score Random Forest")
plt.xlabel("max_depth")
plt.ylabel("Score")
print("Max Random Forest :",max(acc_rf),"pada k =",acc_rf.index(max(acc_rf))+1)  

#test accuracy k=22
rf=RandomForestClassifier(max_depth=22)
model_rf =rf.fit(X_train, y_train)
#evaluate the model
yhat_model_rf=model_rf.predict(X_test)
#evaluate predictions
accuracy_model_rf = accuracy_score(y_test,yhat_model_rf)
recall_model_rf = recall_score(y_test, yhat_model_rf)
precision_model_rf = precision_score(y_test, yhat_model_rf)
f1score_model_rf = f1_score(y_test, yhat_model_rf)
print('Accuracy  : %.3f' % (accuracy_model_rf*100))   
print('Recall    : %.3f' % (recall_model_rf*100))   
print('Precision : %.3f' % (precision_model_rf*100))
print('F1Score   : %.3f' % (f1score_model_rf*100))

random_forest_cm = confusion_matrix(y_test,yhat_model_rf)
sns.heatmap(random_forest_cm,annot=True,fmt="d");

print(classification_report(y_test,yhat_model_rf))

## <span style='color:purple'>Gradient Boosting Classifier</span>

gb=GradientBoostingClassifier(random_state=1)
#evaluate model
scores = evaluate_model(gb)
print('%.3f(%.3f)' % (np.mean(scores)*100,np.std(scores)))

acc_gb = list()
range_i = range(1,51)
for i in range_i :
    gb=GradientBoostingClassifier(max_depth=i,random_state=1)
    #evaluate model
    scores_gb = evaluate_model(gb)
    acc_gb.append(np.mean(scores_gb)*100)
    print('>%d %.3f(%.3f)' % (i,np.mean(scores_gb)*100,np.std(scores_gb)))

plt.figure(figsize=(20,5))
gb_all_features = plt.plot(range(1,51),acc_gb,color='red',marker='o',markerfacecolor='blue',markersize=10)
plt.title("Accuracy Score Gradient Boosting ")
plt.xlabel("max_depth")
plt.ylabel("Score")
print("Max Gradient Boosting :",max(acc_gb),"pada k =",acc_gb.index(max(acc_gb))+1)    

#test accuracy k=2
gb=GradientBoostingClassifier()
model_gb =gb.fit(X_train, y_train)
#evaluate the model
yhat_model_gb=model_gb.predict(X_test)
#evaluate predictions
accuracy_model_gb = accuracy_score(y_test,yhat_model_gb)
recall_model_gb = recall_score(y_test, yhat_model_gb)
precision_model_gb = precision_score(y_test, yhat_model_gb)
f1score_model_gb = f1_score(y_test, yhat_model_gb)
print('Accuracy  : %.3f' % (accuracy_model_gb*100))   
print('Recall    : %.3f' % (recall_model_gb*100))   
print('Precision : %.3f' % (precision_model_gb*100))
print('F1Score   : %.3f' % (f1score_model_gb*100))

gradient_boosting_cm = confusion_matrix(y_test,yhat_model_gb)
sns.heatmap(gradient_boosting_cm,annot=True,fmt="d");

print(classification_report(y_test,yhat_model_gb))

## <span style='color:green'>Naive Bayes</span>

nb=GaussianNB()
#evaluate model
scores = evaluate_model(nb)
print('%.3f(%.3f)' % (np.mean(scores)*100,np.std(scores)))

model_nb= nb.fit(X_train, y_train)
#evaluate the model
yhat_model_nb=model_nb.predict(X_test)
#evaluate predictions
accuracy_model_nb = accuracy_score(y_test,yhat_model_nb)
recall_model_nb = recall_score(y_test, yhat_model_nb)
precision_model_nb = precision_score(y_test, yhat_model_nb)
f1score_model_nb = f1_score(y_test, yhat_model_nb)
print('Accuracy  : %.3f' % (accuracy_model_nb*100))   
print('Recall    : %.3f' % (recall_model_nb*100))   
print('Precision : %.3f' % (precision_model_nb*100))
print('F1Score   : %.3f' % (f1score_model_nb*100))

naive_bayes_cm = confusion_matrix(y_test,yhat_model_nb)
sns.heatmap(naive_bayes_cm,annot=True,fmt="d");

print(classification_report(y_test,yhat_model_nb))

## <span style='color:teal'>Logistic Regression</span>

lr=LogisticRegression(solver='saga',random_state=1)
#evaluate model
scores = evaluate_model(lr)
print('%.3f(%.3f)' % (np.mean(scores)*100,np.std(scores)))

model_lr= lr.fit(X_train, y_train)
#evaluate the model
yhat_model_lr=model_lr.predict(X_test)
#evaluate predictions
accuracy_model_lr = accuracy_score(y_test,yhat_model_lr)
recall_model_lr = recall_score(y_test, yhat_model_lr)
precision_model_lr = precision_score(y_test, yhat_model_lr)
f1score_model_lr = f1_score(y_test, yhat_model_lr)
print('Accuracy  : %.3f' % (accuracy_model_lr*100))   
print('Recall    : %.3f' % (recall_model_lr*100))   
print('Precision : %.3f' % (precision_model_lr*100))
print('F1Score   : %.3f' % (f1score_model_lr*100))

logistic_regression_cm = confusion_matrix(y_test,yhat_model_lr)
sns.heatmap(logistic_regression_cm,annot=True,fmt="d");

print(classification_report(y_test,yhat_model_lr))

## <span style='color:cyan'> Support Vector Machines</span>

sgd = SGDClassifier(random_state=1)
#evaluate model
scores = evaluate_model(sgd)
print('%.3f(%.3f)' % (np.mean(scores)*100,np.std(scores)))

model_sgd= sgd.fit(X_train, y_train)
#evaluate the model
yhat_model_sgd=model_sgd.predict(X_test)
#evaluate predictions
accuracy_model_sgd = accuracy_score(y_test,yhat_model_sgd)
recall_model_sgd = recall_score(y_test, yhat_model_sgd)
precision_model_sgd = precision_score(y_test, yhat_model_sgd)
f1score_model_sgd = f1_score(y_test, yhat_model_sgd)
print('Accuracy  : %.3f' % (accuracy_model_sgd*100))   
print('Recall    : %.3f' % (recall_model_sgd*100))   
print('Precision : %.3f' % (precision_model_sgd*100))
print('F1Score   : %.3f' % (f1score_model_sgd*100))

sgd_cm = confusion_matrix(y_test,yhat_model_sgd)
sns.heatmap(sgd_cm,annot=True,fmt="d");

print(classification_report(y_test,yhat_model_sgd))

## <span style='color:violet'>XGBoost</span>

xgb = XGBClassifier(random_state=42, n_jobs=-1,max_depth=3)
#evaluate model
scores = evaluate_model(xgb)
print('%.3f(%.3f)' % (np.mean(scores)*100,np.std(scores)))

model_xgb= xgb.fit(X_train, y_train)
#evaluate the model
yhat_model_xgb=model_xgb.predict(X_test)
#evaluate predictions
accuracy_model_xgb = accuracy_score(y_test,yhat_model_xgb)
recall_model_xgb = recall_score(y_test, yhat_model_xgb)
precision_model_xgb = precision_score(y_test, yhat_model_xgb)
f1score_model_xgb = f1_score(y_test, yhat_model_xgb)
print('Accuracy  : %.3f' % (accuracy_model_xgb*100))   
print('Recall    : %.3f' % (recall_model_xgb*100))   
print('Precision : %.3f' % (precision_model_xgb*100))
print('F1Score   : %.3f' % (f1score_model_xgb*100))

xgboost_cm = confusion_matrix(y_test,yhat_model_xgb)
sns.heatmap(xgboost_cm,annot=True,fmt="d");

print(classification_report(y_test,yhat_model_xgb))

## <span style='color:purple'>Artificial Neural Network</span>

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=np.array(y))
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25,stratify=np.array(y_train))

#create model
model = Sequential()
model.add(Dense(33,input_dim=95,kernel_initializer='uniform',activation='relu'))
model.add(Dense(12,kernel_initializer='uniform',activation='relu'))
model.add(Dense(3,kernel_initializer='uniform',activation='relu'))
model.add(Dense(1,kernel_initializer='uniform',activation='sigmoid'))

#compile model
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()
plot_model(model,show_shapes=True)
model.save("my_keras_model.h5")
checkpoint_cb = keras.callbacks.ModelCheckpoint("my_keras_model.h5",save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)

#fit the model
history = model.fit(x=X_train,y=y_train,epochs=500,
                    validation_data=(X_val,y_val),callbacks=[checkpoint_cb,early_stopping_cb])
                    
epoch_list = list(range(1,41)) #EPOCH=150
y_train_acc = history.history['accuracy']
y_val_acc = history.history['val_accuracy']
y_train_loss = history.history['loss']
y_val_loss = history.history['val_loss']

f,(ax1,ax2) = plt.subplots(1,2,figsize=(24,4))
t = f.suptitle('Artificial Neural Network',fontsize=12)

ax1.plot(epoch_list,y_train_acc,label='Train Accuracy')
ax1.plot(epoch_list,y_val_acc,label='Validation Accuracy')
#ax1.set_xticks(np.arange(0,12,1))
#ax1.set_ylim(0.75,0.85)
ax1.set_ylabel('Accuracy Value')
ax1.set_xlabel('Epoch')
ax1.set_title('Accuracy')
l1=ax1.legend(loc="best")

ax2.plot(epoch_list,y_train_loss,label='Train Loss')
ax2.plot(epoch_list,y_val_loss,label='Validation Loss')
#ax2.set_xticks(np.arange(0,12,1))
#ax2.set_ylim(0,1)
ax2.set_ylabel('Cross Entropy')
ax2.set_xlabel('Epoch')
ax2.set_title('Loss')
l2=ax2.legend(loc="best")

print("Max ANN Train Acc :",max(y_train_acc),"pada epoch =",y_train_acc.index(max(y_train_acc))+1)
print("Max ANN Validation Acc :",max(y_val_acc),"pada epoch =",y_val_acc.index(max(y_val_acc))+1)
print("Max ANN Train Loss :",max(y_train_loss),"pada epoch =",y_train_loss.index(max(y_train_loss))+1)
print("Max ANN Validation Loss :",max(y_val_loss),"pada epoch =",y_val_loss.index(max(y_val_loss))+1)

y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5)

#evaluate predictions
accuracy_model_ann = accuracy_score(y_test,y_pred)
recall_model_ann = recall_score(y_test, y_pred)
precision_model_ann = precision_score(y_test, y_pred)
f1score_model_ann = f1_score(y_test, y_pred)
print('Accuracy  : %.3f' % (accuracy_model_ann*100))   
print('Recall    : %.3f' % (recall_model_ann*100))   
print('Precision : %.3f' % (precision_model_ann*100))
print('F1Score   : %.3f' % (f1score_model_ann*100))

ann_cm = confusion_matrix(y_test,y_pred)
sns.heatmap(ann_cm,annot=True,fmt="d");

print(classification_report(y_test,y_pred))
